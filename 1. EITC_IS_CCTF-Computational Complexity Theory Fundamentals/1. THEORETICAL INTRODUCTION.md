## THEORETICAL INTRODUCTION

### EN:
## INTRODUCTION
Computational complexity theory is a fundamental field in computer science that deals with the study of the 
resources required to solve computational problems. In the context of cybersecurity, understanding the 
computational complexity of algorithms and cryptographic protocols is important for assessing their security 
and efficiency. This didactic material aims to provide a theoretical introduction to computational complexity 
theory, focusing on its relevance to cybersecurity. 

At its core, computational complexity theory investigates the inherent difficulty of solving problems on various 
computational models, such as Turing machines or Boolean circuits. It seeks to classify problems based on their 
computational complexity, which is typically measured in terms of time and space requirements. By analyzing 
the complexity of algorithms, we can gain insights into the feasibility and efficiency of solving specific problems. 

One of the key concepts in computational complexity theory is the notion of a computational problem. A 
computational problem defines a set of instances, each of which has a corresponding solution. For example, the 
problem of sorting a list of numbers can be seen as a computational problem, where the instances are the 
different lists, and the solutions are the sorted versions of those lists. 

To analyze the complexity of computational problems, complexity classes are introduced. Complexity classes 
group problems based on their computational resources required for their solution. One of the most well-known 
complexity classes is P, which contains problems that can be solved efficiently in polynomial time. In contrast, 
the class NP includes problems that can be verified efficiently but may not be solved efficiently. The relationship 
between P and NP is one of the most important open questions in computer science and has significant 
implications for cybersecurity. 

In the context of cybersecurity, understanding the complexity of cryptographic algorithms is important for 
assessing their security. For example, the security of many encryption schemes relies on the assumption that 
certain problems are computationally hard to solve. If an attacker can efficiently solve these problems, the 
security of the encryption scheme is compromised. Therefore, analyzing the computational complexity of 
cryptographic algorithms helps in evaluating their resistance against various attacks. 

Moreover, computational complexity theory provides tools for analyzing the efficiency of algorithms used in 
cybersecurity applications. By determining the complexity of an algorithm, we can estimate its running time and 
space requirements. This analysis helps in selecting efficient algorithms for various cybersecurity tasks, such as 
intrusion detection, malware analysis, or secure communication protocols.

Computational complexity theory is a fundamental field in computer science with significant implications for 
cybersecurity. It allows us to analyze the inherent difficulty of computational problems, classify them based on 
their complexity, and evaluate the efficiency and security of algorithms and cryptographic protocols. By 
understanding the computational complexity of these systems, we can make informed decisions in designing 
secure and efficient cybersecurity solutions.

## DETAILED DIDACTIC MATERIAL
Welcome to this didactic material on the fundamentals of computational complexity theory in the field of 
cybersecurity. In this material, we will provide a theoretical introduction to the topic.

Before we consider the content, it is important to have some background knowledge. In this series of materials, we will be discussing sets extensively. Therefore, it is important to understand the notation we will be using. If you encounter any unfamiliar concepts in this material, it is recommended to review the relevant material from any relevant source before proceeding, for example from Wikipedia: 
https://en.wikipedia.org/wiki/Set_(mathematics). 

Let us begin by discussing sets. We will represent the set of natural numbers, which includes positive integers starting from 1, using the symbol ℕ. This set is infinite. Additionally, we may also encounter negative numbers, which can be represented by the symbol ℤ, denoting the set of all integers. The symbol Ø with a slash through it represents the empty set, which has no elements. We may also use a pair of braces {} to indicate a set with zero elements.

In the realm of sets, we have various operations at our disposal. We can perform union and intersection operations on sets. Furthermore, we can also inquire about the elements not present in a set. However, it is important to note that these operations assume the existence of a universe of elements from which we can identify the elements that may or may not be part of a set. 

To illustrate the concept of cross products, we use a symbol that resembles an X. The cross product of two sets, denoted as set S and set T, forms a pair of elements, with one element drawn from set S and the other from set T. For example, if we take the cross product of the set ℕ with itself (ℕ × ℕ), we are referring to pairs of natural numbers, i.e., pairs of positive integers (i, j) where both i and j are greater than or equal to one. 

In our discussions, we will often encounter tuples represented in the following format: {element | constraints}. Here, the braces {} indicate a set, an L represents an element, and the vertical bar | separates the element from the additional constraints. For instance, a tuple with two elements will be included in the set if the first element is greater than or equal to one and the second element is also greater than or equal to one. 

Sequences of symbols are another concept we will explore. If we have a set, we can inquire about the set of all its subsets, known as the power set. The power set of set S is denoted as P(S) and encompasses all possible subsets of S.

To aid our understanding of sets, we can utilize Venn diagrams, which visually represent the intersection of two sets.

Functional notation is essential in our discussions. Functions, denoted as f, take elements from one set, known as the domain set, and map them to another set, called the range set. For example, if we have a function f that takes a value X from the domain and yields a value Y in the range, we can express it as f(X) = Y. Functions can be unary, taking a single argument, or they can be binary or higher arity, taking two or more arguments. 

In terms of notation, functions can be represented using either prefix or infix notation. Prefix notation places the function symbol before its arguments, while infix notation places the function symbol between its arguments. For instance, the negation operation (-a) uses prefix notation, while the addition operator (a + b) uses infix notation. 

When dealing with binary or higher arity functions, the domain becomes a set of tuples. For example, a binary relation G may take two natural numbers as arguments, and its domain can be represented as ℕ × ℕ, indicating that it takes two arguments and yields a single natural number as a result. In some cases, the range can be the boolean set, consisting of true or false values. Functions that map elements from the domain to true or false are called predicates. Predicates that take a single argument are referred to as properties, while those that take multiple arguments are known as relations. 

An example of a property is the "is odd" property, which can be applied to any integer. When applied to the number 4, the property yields false, while when applied to the number 5, it yields true. This demonstrates that the property "is odd" is a predicate that takes one argument.

This concludes our theoretical introduction to computational complexity theory in the field of cybersecurity. Stay tuned for more materials that will delve deeper into the topic. 

A binary relation is a type of relation that takes two arguments and returns either true or false. It is commonly represented using functional notation or infix notation. For example, the less than relation can be represented as X is less than Y. 

Binary relations can have certain properties. A reflexive relation is one where every element is related to itself. An example of a reflexive relation is the equal relation, where X is always equal to itself. On the other hand, an irreflexive relation is one where no element is related to itself.

Symmetric properties in binary relations imply that if one element is related to another, then the reverse is also true. For example, the equal relation is symmetric because if X equals Y, then Y also equals X. However, the less than relation is not symmetric because if X is less than Y, it does not imply that Y is less than X. 

Transitive relations are those where if X is related to Y and Y is related to Z, then X is related to Z. The less than relation is transitive because if X is less than Y and Y is less than Z, then X is less than Z. 

In the context of computational complexity theory, graphs play a important role. A graph consists of vertices (also called nodes) and edges that connect them. The edges can be directed or undirected, and the nodes and edges can be labeled or unlabeled. 

Subgraphs refer to a subset of nodes and the edges that connect them, ignoring the remaining nodes and edges of the larger graph. Connected components are subgraphs where all nodes are connected to each other, while unconnected components have nodes that are not connected to each other. 

Paths in a graph refer to a sequence of nodes connected by edges. In a directed graph, the path must follow the direction of the edges. Cycles occur when a path forms a closed loop, returning to the starting node. 

In directed graphs, the in-degree of a node refers to the number of edges that come into that node, while the out-degree refers to the number of edges that go out of that node.

There is a strong connection between binary relations and directed graphs. Each node in the graph corresponds to an element in the domain of the binary relation. If the relation holds between two elements, there will be a directed edge from one node to another in the graph. Therefore, there is a one-to-one correspondence between directed graphs and binary relations. 

Understanding these fundamental concepts of computational complexity theory is essential for studying cybersecurity and analyzing the efficiency and complexity of algorithms. 

In the field of cybersecurity, it is important to understand the fundamentals of computational complexity theory. One key concept in this theory is the study of graphs, specifically trees. A tree is a special type of graph with directed edges. These edges indicate a definite direction, with the arrowhead pointing from the parent node to the child node. In a tree, there are no cycles, meaning that it is not possible to go back up from a child node to a parent node. Additionally, a tree has a distinguished root node and leaves, which have an out degree of 0, meaning they have no edges going out. The interior nodes of a tree can have a degree greater than 1. 

Another type of directed graph is known as a directed acyclic graph (DAG). Similar to a tree, a DAG has directed edges and no cycles. However, a DAG can have multiple root nodes and nodes with multiple parents. This allows for more flexibility in the structure of the graph.

In the context of computational complexity theory, strings play a significant role. Before defining a string, it is important to establish an alphabet, which is a finite set of symbols. For example, an alphabet could consist of the letters A, B, C, and D. A string is then defined as a sequence of symbols, with a first symbol and a last symbol. Importantly, a string is finite, meaning it has a definite length and a finite number of symbols. The length of a string can be determined by counting the number of symbols it contains. It is also possible to have an empty string, denoted by the lowercase epsilon symbol (ε), which has a length of zero. 

Concatenation is another operation that can be performed on strings. It involves combining two strings together by placing them next to each other. 

Understanding these fundamental concepts of computational complexity theory, such as trees, DAGs, and strings, is important in the field of cybersecurity. These concepts provide a foundation for analyzing and solving complex problems related to computational systems and algorithms. 

A language is a set of strings over some alphabet. For example, if we have the alphabet Sigma, we can define a language L1 consisting of four strings, each with a length of two. Another example is the language L2, which






### ES:
## Fundamentos de la Teoría de la Complejidad Computacional

La teoría de la complejidad computacional es un campo fundamental de la informática que se ocupa del 
estudio de los recursos necesarios para resolver problemas computacionales. En el contexto de la 
ciberseguridad, comprender la complejidad computacional de los algoritmos y los protocolos criptográficos 
es importante para evaluar su seguridad y eficiencia. Este material didáctico tiene como objetivo 
proporcionar una introducción teórica a la teoría de la complejidad computacional, centrándose en su 
relevancia para la ciberseguridad.

En esencia, la teoría de la complejidad computacional investiga la dificultad inherente de resolver problemas 
en varios modelos computacionales, como las máquinas de Turing o los circuitos booleanos. Busca clasificar 
los problemas en función de su complejidad computacional, que normalmente se mide en términos de 
requisitos de tiempo y espacio. Analizando la complejidad de los algoritmos, podemos obtener información 
sobre la viabilidad y eficiencia de resolver problemas específicos.

Uno de los conceptos clave en la teoría de la complejidad computacional es la noción de problema 
computacional. Un problema computacional define un conjunto de instancias, cada una de las cuales tiene 
una solución correspondiente. Por ejemplo, el problema de ordenar una lista de números puede verse como 
un problema computacional, donde las instancias son las diferentes listas y las soluciones son las versiones
ordenadas de esas listas.

Para analizar la complejidad de los problemas computacionales, se introducen las clases de complejidad. Las 
clases de complejidad agrupan problemas según los recursos computacionales necesarios para su solución. 
Una de las clases de complejidad más conocidas es P, que contiene problemas que pueden resolverse de 
forma eficiente en tiempo polinómico. En cambio, la clase NP incluye problemas que pueden verificarse 
eficientemente pero que pueden no resolverse de forma eficiente. La relación entre P y NP es una de las 
cuestiones abiertas más importantes de la informática y tiene importantes implicaciones para la ciberseguridad.

En el contexto de la ciberseguridad, comprender la complejidad de los algoritmos criptográficos es 
importante para evaluar su seguridad. Por ejemplo, la seguridad de muchos esquemas de cifrado se basa en 
la suposición de que ciertos problemas son computacionalmente difíciles de resolver. Si un atacante puede 
resolver estos problemas de forma eficiente, la seguridad del esquema de cifrado queda comprometida. Por 
lo tanto, analizar la complejidad computacional de los algoritmos criptográficos ayuda a evaluar su 
resistencia frente a diversos ataques.

Además, la teoría de la complejidad computacional proporciona herramientas para analizar la eficiencia 
de los algoritmos utilizados en aplicaciones de ciberseguridad. Al determinar la complejidad de un algoritmo, 
podemos estimar su tiempo de ejecución y requisitos de espacio. Este análisis ayuda a seleccionar 
algoritmos eficientes para diversas tareas de ciberseguridad, como la detección de intrusiones, el análisis de 
malware o los protocolos de comunicación segura.

La teoría de la complejidad computacional es un campo fundamental de la informática con importantes 
implicaciones para la ciberseguridad. Nos permite analizar la dificultad inherente de los problemas 
computacionales, clasificarlos en función de su complejidad y evaluar la eficiencia y seguridad de los 
algoritmos y protocolos criptográficos. Comprendiendo la complejidad computacional de estos sistemas, 
podemos tomar decisiones fundamentadas al diseñar soluciones de ciberseguridad seguras y eficientes.
